{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered\">\n",
    "    <tr>\n",
    "        <th style=\"width:200px;\">\n",
    "            <img src='https://bcgriseacademy.com/hs-fs/hubfs/RISE%202.0%20Logo_Options_25Jan23_RISE%20-%20For%20Black%20Background.png?width=3522&height=1986&name=RISE%202.0%20Logo_Options_25Jan23_RISE%20-%20For%20Black%20Background.png' style=\"background-color:black; width: 100%; height: 100%;\">\n",
    "        </th>\n",
    "        <th style=\"text-align:center;\">\n",
    "            <h1>IBF TFIP</h1>\n",
    "            <h2>Regression with Scikitlearn</h2>\n",
    "        </th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Learning Objectives](#toc0_)\n",
    "#### <a id='toc1_1_1_1_'></a>[After completing this lesson, you should be able to:](#toc0_)\n",
    "1. LO1: Evaluate a Regression model\n",
    "2. LO2: Understand Regression coefficients\n",
    "3. LO3: Validate a model by removing variance and bias\n",
    "4. LO4: When to use Polynomial Regression over Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>      \n",
    "  - [1 Model Training & Statistics(Part II)](#toc1_2_)    \n",
    "    - [1.1 Model Evaluation](#toc1_2_1_)    \n",
    "      - [1.1.1 Unstandardized Regression Coefficients](#toc1_2_1_1_)    \n",
    "      - [1.1.2 Standardized Regression Coefficients](#toc1_2_1_2_)    \n",
    "      - [1.1.3 R squared](#toc1_2_1_3_)    \n",
    "      - [1.1.4 Adj R squared](#toc1_2_1_4_)    \n",
    "      - [1.1.5 p-values](#toc1_2_1_5_)    \n",
    "      - [1.1.6 Checking linearity assumptions](#toc1_2_1_6_)    \n",
    "      - [1.1.7 Regression Evaluation Metrics](#toc1_2_1_7_)    \n",
    "    - [1.2 Model Validation](#toc1_2_2_)    \n",
    "      - [1.2.1 Variance vs Bias](#toc1_2_2_1_)    \n",
    "        - [1.2.1.1 Low Variance - Low Bias](#toc1_2_2_1_1_)    \n",
    "        - [1.2.1.2 Low Variance - High Bias](#toc1_2_2_1_2_)    \n",
    "        - [1.2.1.3 High Variance - Low Bias](#toc1_2_2_1_3_)    \n",
    "        - [1.2.1.4 High Variance - High Bias](#toc1_2_2_1_4_)    \n",
    "        - [1.2.1.5 Learning Curves](#toc1_2_2_1_5_)    \n",
    "    - [1.3 Polynomial Regression](#toc1_2_3_)    \n",
    "      - [1.3.1 Code explanation:](#toc1_2_3_1_)    \n",
    "    - [1.4 Summary](#toc1_2_4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a id='toc1_2_'></a>[1. Model Training & Statistics(Part II)](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <a id='toc1_2_1_'></a>[1.1 Model Evaluation](#toc0_)\n",
    "    \n",
    "Simply, building a predictive model is not our motive. But, creating and selecting a model which gives high accuracy on out of sample data. Hence, it is crucial to check accuracy of the model prior to computing predicted values ([Linear Regression Metrics](https://dziganto.github.io/data%20science/linear%20regression/machine%20learning/python/Linear-Regression-101-Metrics/)). After you are finished building your model, these are some of the important metrics that will help you in [evaluating](https://www.coursera.org/lecture/machine-learning-with-python/model-evaluation-in-regression-models-2WZdq) your model accuracy.  \n",
    "\n",
    "#### <a id='toc1_2_1_1_'></a>[1.1.1 Unstandardized Regression Coefficients](#toc0_)\n",
    "\n",
    "One example would be a model of the height of a shrub (Y) based on the amount of bacteria in the soil (X₁) and whether the plant is located in partial or full sun (X₂).\n",
    "\n",
    "Height is measured in cm, bacteria is measured in thousand per ml of soil, and type of sun = 0 if the plant is in partial sun and type of sun = 1 if the plant is in full sun.\n",
    "\n",
    "Let’s say it turned out that the regression equation was estimated as follows:\n",
    "\n",
    "\n",
    "Y = 42 + 2.3*X₁ + 11*X₂\n",
    "\n",
    "( __Regression Equation : Y = β₁ + β₂X₁ + β₃X₂ +.. + βₚXₚ₋₁ + ϵ__)\n",
    "\n",
    "* <b>Interpreting Coefficients of Continuous Predictor Variables</b>  \n",
    "<br>\n",
    "    Since X1 is a continuous variable, β₂(coefficient of X₁) represents the difference in the predicted value of Y for each one-unit difference in X₁, if X₂ remains constant.This means that if X₁ differed by one unit (and X₂ did not differ) Y will differ by β₂ units, on average.\n",
    "    \n",
    "    Taking an example, shrubs with a 5000 bacteria count would, on average, be 2.3 cm taller than those with a 4000/ml bacteria count, which likewise would be about 2.3 cm taller than those with 3000/ml bacteria, as long as they were in the same type of sun.\n",
    "\n",
    "    *Don’t forget that since the bacteria count was measured in 1000 per ml of soil, 1000 bacteria represent one unit of X₁*.\n",
    "<br>\n",
    "\n",
    "* <b>Interpreting Coefficients of Categorical Predictor Variables</b> \n",
    "<br>\n",
    "\n",
    "    <mark>Similarly, β₃ is interpreted as the difference in the predicted value in Y for each one-unit difference in X₂ if X₁ remains constant. However, since X₂ is a categorical variable coded as 0 or 1, a one unit difference represents switching from one category to the other.\n",
    "\n",
    "    <mark>β3 is then the average difference in Y between the category for which X₂ = 0 (the reference group) and the category for which X₂ = 1 (the comparison group).\n",
    "\n",
    "    <mark>So compared to shrubs that were in partial sun, we would expect shrubs in full sun to be 11 cm taller, on average, at the same level of soil bacteria.\n",
    "<br>\n",
    "\n",
    "* <b>Interpreting Coefficients when Predictor Variables are Correlated</b> \n",
    "<br>\n",
    "\n",
    "    Don’t forget that each coefficient is influenced by the other variables in a regression model. Because predictor variables are nearly always associated, two or more variables may explain some of the same variation in Y.\n",
    "\n",
    "    Therefore, each coefficient does not measure the total effect on Y of its corresponding variable, as it would if it were the only variable in the model.\n",
    "\n",
    "    Rather, each coefficient represents the additional effect of adding that variable to the model, if the effects of all other variables in the model are already accounted for. (This is called Type 3 regression coefficients and is the usual way to calculate them. However, not all software uses Type 3 coefficients, so make sure you check your software manual so you know what you’re getting).\n",
    "\n",
    "#### <a id='toc1_2_1_2_'></a>[1.1.2 Standardized Regression Coefficients](#toc0_)\n",
    "\n",
    "While building a regression model you may face circumstances where you can find a variable whose unstandardized regression coefficient (aka beta or estimate) is close to zero (.0003) but it is statistically significant (p-value < .05). If a variable is significant, it means its coefficient value is significantly different from zero. The question arises __\"Why coefficient value is close to zero if it is a significant variable?__\n",
    "\n",
    "The answer lies in the __difference between unstandardized coefficient and standardized coefficient__\n",
    "\n",
    "<mark>If an independent variable is expressed in millions or billions of Rupees (for eg, Rs.656,765), it can have unstandardized estimate close to zero. To make the coefficient value more interpretable, we can rescale the variable by dividing the variable by 1000 or 100,000 (depending on the value). After rescaling the variable, run regression analysis again including the transformed variable. You would find beta coefficient larger than the old coefficient value and significantly larger than 0.  \n",
    "\n",
    "__Detailed Explanation__\n",
    "\n",
    "The concept of standardization or standardized coefficients comes into picture when predictors (aka independent variables) are expressed in different units. Suppose you have 3 independent variables - age, height and weight. The variable 'age' is expressed in years, height in cm, weight in kg. If we need to rank these predictors based on the unstandardized coefficient, it would not be a fair comparison as the unit of these variable is not same.\n",
    "\n",
    "__Real Use of Standardized Coefficients__\n",
    "\n",
    "They are mainly used to rank predictors (or independent or explanatory variables) as it eliminate the units of measurement of  independent and dependent variables). We can rank independent variables with absolute value of standardized coefficients. The most important variable will have maximum absolute value of standardized coefficient.\n",
    "\n",
    "__Calculation of Standardized Coefficients__\n",
    "\n",
    "Standardize both dependent and independent variables and use the standardized variables in the regression model to get standardized estimates. By 'standardize', i mean subtract the mean from each observation and divide that by the standard deviation. It is also called z-score. It would make mean 0 and standard deviation 1.\n",
    "\n",
    "OR \n",
    "\n",
    "You can multiplying the unstandardized coefficient by the ratio of the standard deviations of the independent variable and dependent variable.\n",
    "\n",
    "\n",
    "#### <a id='toc1_2_1_3_'></a>[1.1.3 R squared](#toc0_)\n",
    "\n",
    "R squared also known as Coefficient of determination is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.\n",
    "\n",
    "The definition of R-squared is fairly straight-forward; it is the percentage of the response variable variation that is explained by a linear model. Or:\n",
    "\n",
    "R-squared = Explained variation / Total variation\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "\n",
    "<b>0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "100% indicates that the model explains all the variability of the response data around its mean.</b>\n",
    "In general, the higher the R-squared, the better the model fits your data.\n",
    "\n",
    "<b>Key Limitations of R-squared</b>\n",
    "\n",
    "1. R-squared cannot determine whether the coefficient estimates and predictions are biased, which is why you must assess the residual plots.  \n",
    "\n",
    "2. R-squared does not indicate whether a regression model is adequate. You can have a low R-squared value for a good model, or a high R-squared value for a model that does not fit the data.\n",
    "\n",
    "<b> Are Low R-squared Values Inherently Bad? </b>\n",
    "\n",
    "No! There are two major reasons why it can be just fine to have low R-squared values.\n",
    "\n",
    "In some fields, it is entirely expected that your R-squared values will be low. For example, any field that attempts to predict human behavior, such as psychology, typically has R-squared values lower than 50%. Humans are simply harder to predict than, say, physical processes.\n",
    "\n",
    "Furthermore, if your R-squared value is low but you have statistically significant predictors, you can still draw important conclusions about how changes in the predictor values are associated with changes in the response value. Regardless of the R-squared, the significant coefficients still represent the mean change in the response for one unit of change in the predictor while holding other predictors in the model constant. Obviously, this type of information can be extremely valuable.\n",
    "\n",
    "A low R-squared doesn't affect the interpretation of significant variables. A low R-squared is most problematic when you want to produce predictions that are reasonably precise (have a small enough prediction interval). How high should the R-squared be for prediction? Well, that depends on your requirements for the width of a prediction interval and how much variability is present in your data. While a high R-squared is required for precise predictions, it’s not sufficient by itself, as we shall see.\n",
    "    \n",
    "\\* More reference in the External Link Library\n",
    "\n",
    "<b> Are High R-squared Values Inherently Good? </b>\n",
    "\n",
    "No! A high R-squared does not necessarily indicate that the model has a good fit. That might be a surprise, but look at the fitted line plot and residual plot below. The fitted line plot displays the relationship between semiconductor electron mobility and the natural log of the density for real experimental data.\n",
    "\n",
    "<img src='images/Rsquare1.png' width=\"500\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "<img src='images/Rsquare2.png' width=\"530\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "The fitted line plot shows that these data follow a nice tight function and the R-squared is 98.5%, which sounds great. However, look closer to see how the regression line systematically over and under-predicts the data (bias) at different points along the curve. You can also see patterns in the Residuals versus Fits plot, rather than the randomness that you want to see. This indicates a bad fit, and serves as a reminder as to why you should always check the residual plots.\n",
    "\n",
    "However, similar biases can occur when your linear model is missing important predictors, polynomial terms, and interaction terms. Statisticians call this specification bias, and it is caused by an underspecified model. For this type of bias, you can fix the residuals by adding the proper terms to the model.\n",
    "\n",
    "\\* More reference in the External Link Library\n",
    "\n",
    "\n",
    "#### <a id='toc1_2_1_4_'></a>[1.1.4 Adj R squared](#toc0_)\n",
    "\n",
    "The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.\n",
    "    \n",
    "\\* More reference in the External Link Library\n",
    "\n",
    "<b>Some Problems with R-squared</b>\n",
    "\n",
    "R-squared cannot determine whether the coefficient estimates and predictions are biased, which is why you must assess the residual plots. However, R-squared has additional problems that the adjusted R-squared and predicted R-squared are designed to address.\n",
    "\n",
    "* <b>Problem 1:</b> Every time you add a predictor to a model, the R-squared increases, even if due to chance alone. It never decreases. Consequently, a model with more terms may appear to have a better fit simply because it has more terms.\n",
    "\n",
    "* <b>Problem 2:</b> If a model has too many predictors and higher order polynomials, it begins to model the random noise in the data. This condition is known as overfitting the model and it produces misleadingly high R-squared values and a lessened ability to make predictions.\n",
    "\n",
    "<b> What is the adjusted R-squared</b>\n",
    "\n",
    "The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors.\n",
    "\n",
    "Suppose you compare a five-predictor model with a higher R-squared to a one-predictor model. Does the five predictor model have a higher R-squared because it’s better? Or is the R-squared higher because it has more predictors? Simply compare the adjusted R-squared values to find out!\n",
    "\n",
    "The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. The adjusted R-squared can be negative, but it’s usually not.  It is always lower than the R-squared.\n",
    "\n",
    "<b> Difference between R-squared and adjusted R-squared</b>\n",
    "\n",
    "* One <b>major difference</b> between R-squared and the adjusted R-squared is that R-squared supposes that every independent variable in the model explains the variation in the dependent variable. It gives the percentage of explained variation as if all independent variables in the model affect the dependent variable. Adjusted R-squared, on the other hand, gives the percentage of variation explained by only those independent variables that in reality affect the dependent variable.\n",
    "<br>\n",
    "\n",
    "* R-squared cannot verify whether the coefficient ballpark figure and its predictions are prejudiced. The adjusted R-squared compares the descriptive power of regression models that include diverse numbers of predictors. The adjusted R-squared compensates for the addition of variables and only increases if the new term enhances the model above what would be obtained by probability and decreases when a predictor enhances the model less than what is predicted by chance.\n",
    "<br>\n",
    "\n",
    "* R-squared also does not show if a regression model is satisfactory; it can show an R-squared figure for a good model, or a high R-squared figure for a model that doesn’t fit.\n",
    "\n",
    "#### <a id='toc1_2_1_5_'></a>[1.1.5 p-values](#toc0_)\n",
    "\n",
    "The p-value is the level of marginal significance within a statistical hypothesis test representing the probability of the occurrence of a given event. The p-value is used as an alternative to rejection points to provide the smallest level of significance at which the [null hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis) would be rejected. A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis. \n",
    "\n",
    "The p-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (< 0.05) indicates that you can reject the null hypothesis. In other words, a predictor that has a low p-value is likely to be a meaningful addition to your model because changes in the predictor's value are related to changes in the response variable.\n",
    "\n",
    "Conversely, a larger (insignificant) p-value suggests that changes in the predictor are not associated with changes in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "You can also develop your linear regression model using Statsmodels package. The benefit of using Statsmodels package is that it provides an extensive list of descriptive statistics, statistical tests, plotting functions, and result statistics that are available for different types of data and each estimator. Statsmodels is built on top of the numerical libraries NumPy and SciPy, integrates with Pandas for data handling and uses Patsy for an R-like formula interface. Statsmodels provides the statistical backend for other Python libraries. Statmodels is free software released under the Modified BSD (3-clause) license.\n",
    "\n",
    "\n",
    "<b>Note that Statsmodels does not add a constant term (recall the factor β1) by default</b>  \n",
    "A constant term can easily be added to the linear regression model. You can do it by X = sm.add_constant(X) (X is the name of the dataframe containing the input (independent variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing train_df1 generated from part 1 module\n",
    "import pandas as pd\n",
    "train_df1= pd.read_csv(r'Input/train_df1.csv')\n",
    "# Importing y_df1 generated from part 1 module\n",
    "y_df1=pd.read_csv(r'Input/y_df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing linear model function from sklearn\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "X = train_df1\n",
    "y = y_df1['Score']\n",
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "# Fitting the linear regression model\n",
    "model = lm.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy.stats.mstats import zscore\n",
    "\n",
    "# Fit and make the predictions by the model\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(list(y), X).fit()\n",
    "predictions = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.595</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.594</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   400.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 27 Jul 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:02:54</td>     <th>  Log-Likelihood:    </th> <td> -27319.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6285</td>      <th>  AIC:               </th> <td>5.469e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6261</td>      <th>  BIC:               </th> <td>5.485e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    23</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                               <td></td>                                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                                                      <td>-2965.0784</td> <td> 1145.113</td> <td>   -2.589</td> <td> 0.010</td> <td>-5209.892</td> <td> -720.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>index</th>                                                      <td>   -0.0020</td> <td>    0.001</td> <td>   -1.961</td> <td> 0.050</td> <td>   -0.004</td> <td>-7.52e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Order</th>                                                      <td>    0.0014</td> <td>    0.001</td> <td>    1.641</td> <td> 0.101</td> <td>   -0.000</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Property Id</th>                                                <td>-1.725e-06</td> <td> 2.33e-07</td> <td>   -7.413</td> <td> 0.000</td> <td>-2.18e-06</td> <td>-1.27e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DOF Gross Floor Area</th>                                       <td>-1.371e-07</td> <td> 1.18e-06</td> <td>   -0.116</td> <td> 0.907</td> <td>-2.45e-06</td> <td> 2.17e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Year Built</th>                                                 <td>   -0.1244</td> <td>    0.009</td> <td>  -14.626</td> <td> 0.000</td> <td>   -0.141</td> <td>   -0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Number of Buildings - Self-reported</th>                        <td>   -0.1019</td> <td>    0.063</td> <td>   -1.613</td> <td> 0.107</td> <td>   -0.226</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Occupancy</th>                                                  <td>   -0.0314</td> <td>    0.040</td> <td>   -0.787</td> <td> 0.431</td> <td>   -0.110</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Site EUI (kBtu/ft²)</th>                                        <td>   -0.6776</td> <td>    0.011</td> <td>  -60.166</td> <td> 0.000</td> <td>   -0.700</td> <td>   -0.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weather Normalized Site Electricity Intensity (kWh/ft²)</th>    <td>   -1.0117</td> <td>    0.071</td> <td>  -14.255</td> <td> 0.000</td> <td>   -1.151</td> <td>   -0.873</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weather Normalized Site Natural Gas Intensity (therms/ft²)</th> <td>   -3.7141</td> <td>    0.969</td> <td>   -3.833</td> <td> 0.000</td> <td>   -5.614</td> <td>   -1.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weather Normalized Site Natural Gas Use (therms)</th>           <td>-4.214e-07</td> <td> 1.57e-06</td> <td>   -0.269</td> <td> 0.788</td> <td> -3.5e-06</td> <td> 2.66e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weather Normalized Site Electricity (kWh)</th>                  <td> 1.239e-06</td> <td> 1.01e-07</td> <td>   12.235</td> <td> 0.000</td> <td> 1.04e-06</td> <td> 1.44e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Water Intensity (All Water Sources) (gal/ft²)</th>              <td> 7.313e-05</td> <td>    0.000</td> <td>    0.167</td> <td> 0.868</td> <td>   -0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Latitude</th>                                                   <td>   13.6203</td> <td>    8.478</td> <td>    1.607</td> <td> 0.108</td> <td>   -2.999</td> <td>   30.240</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Longitude</th>                                                  <td>  -37.6823</td> <td>   11.794</td> <td>   -3.195</td> <td> 0.001</td> <td>  -60.802</td> <td>  -14.563</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Community Board</th>                                            <td>    0.2920</td> <td>    0.085</td> <td>    3.421</td> <td> 0.001</td> <td>    0.125</td> <td>    0.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Council District</th>                                           <td>    0.1082</td> <td>    0.053</td> <td>    2.027</td> <td> 0.043</td> <td>    0.004</td> <td>    0.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Census Tract</th>                                               <td>-7.261e-06</td> <td> 2.05e-05</td> <td>   -0.355</td> <td> 0.723</td> <td>-4.74e-05</td> <td> 3.28e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_Largest Property Use Type - Gross Floor Area (ft²)</th>     <td>   -1.3243</td> <td>    0.559</td> <td>   -2.368</td> <td> 0.018</td> <td>   -2.421</td> <td>   -0.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_Direct GHG Emissions (Metric Tons CO2e)</th>                <td>    1.6924</td> <td>    0.302</td> <td>    5.607</td> <td> 0.000</td> <td>    1.101</td> <td>    2.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_Water Intensity (All Water Sources) (gal/ft²)</th>          <td>   -0.0097</td> <td>    0.338</td> <td>   -0.029</td> <td> 0.977</td> <td>   -0.672</td> <td>    0.652</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Borough_Queens</th>                                             <td>    4.4256</td> <td>    1.259</td> <td>    3.515</td> <td> 0.000</td> <td>    1.957</td> <td>    6.894</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Borough_Staten Island</th>                                      <td>    1.6246</td> <td>    3.002</td> <td>    0.541</td> <td> 0.588</td> <td>   -4.260</td> <td>    7.509</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>333.578</td> <th>  Durbin-Watson:     </th> <td>   1.999</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 634.892</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.393</td>  <th>  Prob(JB):          </th> <td>1.36e-138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.344</td>  <th>  Cond. No.          </th> <td>2.14e+10</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.14e+10. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.595\n",
       "Model:                            OLS   Adj. R-squared:                  0.594\n",
       "Method:                 Least Squares   F-statistic:                     400.1\n",
       "Date:                Thu, 27 Jul 2023   Prob (F-statistic):               0.00\n",
       "Time:                        15:02:54   Log-Likelihood:                -27319.\n",
       "No. Observations:                6285   AIC:                         5.469e+04\n",
       "Df Residuals:                    6261   BIC:                         5.485e+04\n",
       "Df Model:                          23                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================================================================\n",
       "                                                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------------------------\n",
       "const                                                      -2965.0784   1145.113     -2.589      0.010   -5209.892    -720.265\n",
       "index                                                         -0.0020      0.001     -1.961      0.050      -0.004   -7.52e-07\n",
       "Order                                                          0.0014      0.001      1.641      0.101      -0.000       0.003\n",
       "Property Id                                                -1.725e-06   2.33e-07     -7.413      0.000   -2.18e-06   -1.27e-06\n",
       "DOF Gross Floor Area                                       -1.371e-07   1.18e-06     -0.116      0.907   -2.45e-06    2.17e-06\n",
       "Year Built                                                    -0.1244      0.009    -14.626      0.000      -0.141      -0.108\n",
       "Number of Buildings - Self-reported                           -0.1019      0.063     -1.613      0.107      -0.226       0.022\n",
       "Occupancy                                                     -0.0314      0.040     -0.787      0.431      -0.110       0.047\n",
       "Site EUI (kBtu/ft²)                                           -0.6776      0.011    -60.166      0.000      -0.700      -0.655\n",
       "Weather Normalized Site Electricity Intensity (kWh/ft²)       -1.0117      0.071    -14.255      0.000      -1.151      -0.873\n",
       "Weather Normalized Site Natural Gas Intensity (therms/ft²)    -3.7141      0.969     -3.833      0.000      -5.614      -1.815\n",
       "Weather Normalized Site Natural Gas Use (therms)           -4.214e-07   1.57e-06     -0.269      0.788    -3.5e-06    2.66e-06\n",
       "Weather Normalized Site Electricity (kWh)                   1.239e-06   1.01e-07     12.235      0.000    1.04e-06    1.44e-06\n",
       "Water Intensity (All Water Sources) (gal/ft²)               7.313e-05      0.000      0.167      0.868      -0.001       0.001\n",
       "Latitude                                                      13.6203      8.478      1.607      0.108      -2.999      30.240\n",
       "Longitude                                                    -37.6823     11.794     -3.195      0.001     -60.802     -14.563\n",
       "Community Board                                                0.2920      0.085      3.421      0.001       0.125       0.459\n",
       "Council District                                               0.1082      0.053      2.027      0.043       0.004       0.213\n",
       "Census Tract                                               -7.261e-06   2.05e-05     -0.355      0.723   -4.74e-05    3.28e-05\n",
       "log_Largest Property Use Type - Gross Floor Area (ft²)        -1.3243      0.559     -2.368      0.018      -2.421      -0.228\n",
       "log_Direct GHG Emissions (Metric Tons CO2e)                    1.6924      0.302      5.607      0.000       1.101       2.284\n",
       "log_Water Intensity (All Water Sources) (gal/ft²)             -0.0097      0.338     -0.029      0.977      -0.672       0.652\n",
       "Borough_Queens                                                 4.4256      1.259      3.515      0.000       1.957       6.894\n",
       "Borough_Staten Island                                          1.6246      3.002      0.541      0.588      -4.260       7.509\n",
       "==============================================================================\n",
       "Omnibus:                      333.578   Durbin-Watson:                   1.999\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              634.892\n",
       "Skew:                          -0.393   Prob(JB):                    1.36e-138\n",
       "Kurtosis:                       4.344   Cond. No.                     2.14e+10\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.14e+10. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing variables with high p value & with no or little business sense\n",
    "\n",
    "columns = ['Latitude','Longitude','Water Intensity (All Water Sources) (gal/ft²)', 'Number of Buildings - Self-reported','Weather Normalized Site Natural Gas Use (therms)',\n",
    "          'Occupancy','DOF Gross Floor Area','Census Tract', 'Property Id', 'log_Water Intensity (All Water Sources) (gal/ft²)']\n",
    "\n",
    "train_df2 = train_df1.drop(columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit and make the predictions by the model\n",
    "train_df2 = sm.add_constant(train_df2)\n",
    "model_new = sm.OLS(list(y), train_df2).fit()\n",
    "predictions = model_new.predict(train_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.590</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.589</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   694.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 27 Jul 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:02:54</td>     <th>  Log-Likelihood:    </th> <td> -27357.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6285</td>      <th>  AIC:               </th> <td>5.474e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6271</td>      <th>  BIC:               </th> <td>5.484e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                               <td></td>                                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                                                      <td>  383.0406</td> <td>   16.251</td> <td>   23.570</td> <td> 0.000</td> <td>  351.183</td> <td>  414.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>index</th>                                                      <td>   -0.0023</td> <td>    0.001</td> <td>   -2.280</td> <td> 0.023</td> <td>   -0.004</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Order</th>                                                      <td>    0.0015</td> <td>    0.001</td> <td>    1.808</td> <td> 0.071</td> <td>   -0.000</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Year Built</th>                                                 <td>   -0.1326</td> <td>    0.008</td> <td>  -15.685</td> <td> 0.000</td> <td>   -0.149</td> <td>   -0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Site EUI (kBtu/ft²)</th>                                        <td>   -0.6770</td> <td>    0.011</td> <td>  -60.749</td> <td> 0.000</td> <td>   -0.699</td> <td>   -0.655</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weather Normalized Site Electricity Intensity (kWh/ft²)</th>    <td>   -1.0028</td> <td>    0.069</td> <td>  -14.609</td> <td> 0.000</td> <td>   -1.137</td> <td>   -0.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weather Normalized Site Natural Gas Intensity (therms/ft²)</th> <td>   -3.8774</td> <td>    0.944</td> <td>   -4.108</td> <td> 0.000</td> <td>   -5.728</td> <td>   -2.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weather Normalized Site Electricity (kWh)</th>                  <td> 1.301e-06</td> <td> 9.24e-08</td> <td>   14.086</td> <td> 0.000</td> <td> 1.12e-06</td> <td> 1.48e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Community Board</th>                                            <td>    0.2554</td> <td>    0.084</td> <td>    3.040</td> <td> 0.002</td> <td>    0.091</td> <td>    0.420</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Council District</th>                                           <td>    0.0728</td> <td>    0.032</td> <td>    2.255</td> <td> 0.024</td> <td>    0.010</td> <td>    0.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_Largest Property Use Type - Gross Floor Area (ft²)</th>     <td>   -1.2103</td> <td>    0.504</td> <td>   -2.400</td> <td> 0.016</td> <td>   -2.199</td> <td>   -0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_Direct GHG Emissions (Metric Tons CO2e)</th>                <td>    1.6827</td> <td>    0.301</td> <td>    5.587</td> <td> 0.000</td> <td>    1.092</td> <td>    2.273</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Borough_Queens</th>                                             <td>    4.6718</td> <td>    1.140</td> <td>    4.098</td> <td> 0.000</td> <td>    2.437</td> <td>    6.907</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Borough_Staten Island</th>                                      <td>    6.6911</td> <td>    2.595</td> <td>    2.579</td> <td> 0.010</td> <td>    1.604</td> <td>   11.778</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>336.953</td> <th>  Durbin-Watson:     </th> <td>   1.997</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 629.833</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.402</td>  <th>  Prob(JB):          </th> <td>1.71e-137</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.326</td>  <th>  Cond. No.          </th> <td>2.66e+08</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.66e+08. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.590\n",
       "Model:                            OLS   Adj. R-squared:                  0.589\n",
       "Method:                 Least Squares   F-statistic:                     694.9\n",
       "Date:                Thu, 27 Jul 2023   Prob (F-statistic):               0.00\n",
       "Time:                        15:02:54   Log-Likelihood:                -27357.\n",
       "No. Observations:                6285   AIC:                         5.474e+04\n",
       "Df Residuals:                    6271   BIC:                         5.484e+04\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================================================================\n",
       "                                                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------------------------\n",
       "const                                                        383.0406     16.251     23.570      0.000     351.183     414.898\n",
       "index                                                         -0.0023      0.001     -2.280      0.023      -0.004      -0.000\n",
       "Order                                                          0.0015      0.001      1.808      0.071      -0.000       0.003\n",
       "Year Built                                                    -0.1326      0.008    -15.685      0.000      -0.149      -0.116\n",
       "Site EUI (kBtu/ft²)                                           -0.6770      0.011    -60.749      0.000      -0.699      -0.655\n",
       "Weather Normalized Site Electricity Intensity (kWh/ft²)       -1.0028      0.069    -14.609      0.000      -1.137      -0.868\n",
       "Weather Normalized Site Natural Gas Intensity (therms/ft²)    -3.8774      0.944     -4.108      0.000      -5.728      -2.027\n",
       "Weather Normalized Site Electricity (kWh)                   1.301e-06   9.24e-08     14.086      0.000    1.12e-06    1.48e-06\n",
       "Community Board                                                0.2554      0.084      3.040      0.002       0.091       0.420\n",
       "Council District                                               0.0728      0.032      2.255      0.024       0.010       0.136\n",
       "log_Largest Property Use Type - Gross Floor Area (ft²)        -1.2103      0.504     -2.400      0.016      -2.199      -0.222\n",
       "log_Direct GHG Emissions (Metric Tons CO2e)                    1.6827      0.301      5.587      0.000       1.092       2.273\n",
       "Borough_Queens                                                 4.6718      1.140      4.098      0.000       2.437       6.907\n",
       "Borough_Staten Island                                          6.6911      2.595      2.579      0.010       1.604      11.778\n",
       "==============================================================================\n",
       "Omnibus:                      336.953   Durbin-Watson:                   1.997\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              629.833\n",
       "Skew:                          -0.402   Prob(JB):                    1.71e-137\n",
       "Kurtosis:                       4.326   Cond. No.                     2.66e+08\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.66e+08. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing out model summary\n",
    "\n",
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_1_6_'></a>[1.1.6 Checking linearity assumptions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lagrange', 683.4570434104302),\n",
       " ('p-value', 1.1809776712622922e-137),\n",
       " ('f-value', 58.85684811710771),\n",
       " ('f p-value', 5.151516808628059e-146)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for heteroscedasticity \n",
    "\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.compat import lzip\n",
    "\n",
    "name = ['Lagrange','p-value', \n",
    "        'f-value', 'f p-value']\n",
    "test = sms.het_breuschpagan(model_new.resid, model_new.model.exog)\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__As you can see here that the p-value of this test is < 0.05, therefore we can reject the null hypothesis that the variance of the residuals is constant and infer that heteroscedasticity is indeed present, hence we need to perform further iterations to remove it.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping 'const' at index: 0\n",
      "dropping 'Order' at index: 1\n",
      "dropping 'log_Largest Property Use Type - Gross Floor Area (ft²)' at index: 8\n",
      "dropping 'Year Built' at index: 1\n",
      "dropping 'index' at index: 0\n",
      "dropping 'log_Direct GHG Emissions (Metric Tons CO2e)' at index: 6\n",
      "dropping 'Site EUI (kBtu/ft²)' at index: 0\n",
      "dropping 'Community Board' at index: 3\n",
      "dropping 'Weather Normalized Site Natural Gas Intensity (therms/ft²)' at index: 1\n",
      "Remaining variables:\n",
      "Index(['Weather Normalized Site Electricity Intensity (kWh/ft²)',\n",
      "       'Weather Normalized Site Electricity (kWh)', 'Council District',\n",
      "       'Borough_Queens', 'Borough_Staten Island'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check for multicollinearity\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n",
    "\n",
    "def calculate_vif_(X, thresh=2.0):\n",
    "    variables = list(range(X.shape[1]))\n",
    "    drop_cols = list()\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        vif = [variance_inflation_factor(X.iloc[:, variables].values, ix)\n",
    "               for ix in range(X.iloc[:, variables].shape[1])]\n",
    "\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            print('dropping \\'' + X.iloc[:, variables].columns[maxloc] +\n",
    "                  '\\' at index: ' + str(maxloc))\n",
    "            drop_cols.append(X.iloc[:, variables].columns[maxloc])\n",
    "            del variables[maxloc]\n",
    "            dropped = True\n",
    "\n",
    "    print('Remaining variables:')\n",
    "    print(X.columns[variables])\n",
    "    return X.iloc[:, variables], drop_cols\n",
    "\n",
    "X_train, drop_cols = calculate_vif_(train_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing variables with multicollinearity\n",
    "\n",
    "columns = ['const','log_Largest Property Use Type - Gross Floor Area (ft²)','Year Built', 'Order', 'Community Board']\n",
    "\n",
    "train_df3 = train_df2.drop(columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit and make the predictions by the model\n",
    "model_new1 = sm.OLS(list(y), train_df3).fit()\n",
    "\n",
    "# Once you fit the model using .fit() function, you can predict the values using .predict().\n",
    "# In reality once the model has been fit, the scoring(prediction can be performed on new data). Currently we are trying to predict the same dataset\n",
    "predictions = model_new1.predict(train_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.845</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.845</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   3809.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 27 Jul 2023</td> <th>  Prob (F-statistic):</th>           <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:02:55</td>     <th>  Log-Likelihood:    </th>          <td> -29525.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6285</td>      <th>  AIC:               </th>          <td>5.907e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6276</td>      <th>  BIC:               </th>          <td>5.913e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     9</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                               <td></td>                                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>index</th>                                                      <td>    0.0037</td> <td>    0.000</td> <td>   17.958</td> <td> 0.000</td> <td>    0.003</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Site EUI (kBtu/ft²)</th>                                        <td>   -0.7525</td> <td>    0.015</td> <td>  -50.818</td> <td> 0.000</td> <td>   -0.782</td> <td>   -0.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weather Normalized Site Electricity Intensity (kWh/ft²)</th>    <td>    1.1207</td> <td>    0.085</td> <td>   13.157</td> <td> 0.000</td> <td>    0.954</td> <td>    1.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weather Normalized Site Natural Gas Intensity (therms/ft²)</th> <td>  -12.1225</td> <td>    1.319</td> <td>   -9.189</td> <td> 0.000</td> <td>  -14.709</td> <td>   -9.536</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weather Normalized Site Electricity (kWh)</th>                  <td> 3.912e-07</td> <td> 1.06e-07</td> <td>    3.689</td> <td> 0.000</td> <td> 1.83e-07</td> <td> 5.99e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Council District</th>                                           <td>   -0.0448</td> <td>    0.040</td> <td>   -1.110</td> <td> 0.267</td> <td>   -0.124</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_Direct GHG Emissions (Metric Tons CO2e)</th>                <td>   17.2557</td> <td>    0.227</td> <td>   75.933</td> <td> 0.000</td> <td>   16.810</td> <td>   17.701</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Borough_Queens</th>                                             <td>  -12.1578</td> <td>    1.369</td> <td>   -8.882</td> <td> 0.000</td> <td>  -14.841</td> <td>   -9.475</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Borough_Staten Island</th>                                      <td>  -17.8602</td> <td>    3.400</td> <td>   -5.253</td> <td> 0.000</td> <td>  -24.526</td> <td>  -11.195</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>119.155</td> <th>  Durbin-Watson:     </th> <td>   1.954</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 250.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.004</td>  <th>  Prob(JB):          </th> <td>5.15e-55</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.977</td>  <th>  Cond. No.          </th> <td>3.95e+07</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[3] The condition number is large, 3.95e+07. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                      y   R-squared (uncentered):                   0.845\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.845\n",
       "Method:                 Least Squares   F-statistic:                              3809.\n",
       "Date:                Thu, 27 Jul 2023   Prob (F-statistic):                        0.00\n",
       "Time:                        15:02:55   Log-Likelihood:                         -29525.\n",
       "No. Observations:                6285   AIC:                                  5.907e+04\n",
       "Df Residuals:                    6276   BIC:                                  5.913e+04\n",
       "Df Model:                           9                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================================================================\n",
       "                                                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------------------------\n",
       "index                                                          0.0037      0.000     17.958      0.000       0.003       0.004\n",
       "Site EUI (kBtu/ft²)                                           -0.7525      0.015    -50.818      0.000      -0.782      -0.723\n",
       "Weather Normalized Site Electricity Intensity (kWh/ft²)        1.1207      0.085     13.157      0.000       0.954       1.288\n",
       "Weather Normalized Site Natural Gas Intensity (therms/ft²)   -12.1225      1.319     -9.189      0.000     -14.709      -9.536\n",
       "Weather Normalized Site Electricity (kWh)                   3.912e-07   1.06e-07      3.689      0.000    1.83e-07    5.99e-07\n",
       "Council District                                              -0.0448      0.040     -1.110      0.267      -0.124       0.034\n",
       "log_Direct GHG Emissions (Metric Tons CO2e)                   17.2557      0.227     75.933      0.000      16.810      17.701\n",
       "Borough_Queens                                               -12.1578      1.369     -8.882      0.000     -14.841      -9.475\n",
       "Borough_Staten Island                                        -17.8602      3.400     -5.253      0.000     -24.526     -11.195\n",
       "==============================================================================\n",
       "Omnibus:                      119.155   Durbin-Watson:                   1.954\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              250.006\n",
       "Skew:                           0.004   Prob(JB):                     5.15e-55\n",
       "Kurtosis:                       3.977   Cond. No.                     3.95e+07\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[3] The condition number is large, 3.95e+07. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Weather Normalized Site Electricity (kWh) due to high p value\n",
    "\n",
    "train_df4 = train_df3.drop('Weather Normalized Site Electricity (kWh)', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared (uncentered):</th>      <td>   0.845</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.845</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   4275.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 27 Jul 2023</td> <th>  Prob (F-statistic):</th>           <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:02:55</td>     <th>  Log-Likelihood:    </th>          <td> -29532.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  6285</td>      <th>  AIC:               </th>          <td>5.908e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  6277</td>      <th>  BIC:               </th>          <td>5.913e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                               <td></td>                                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>index</th>                                                      <td>    0.0036</td> <td>    0.000</td> <td>   17.676</td> <td> 0.000</td> <td>    0.003</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Site EUI (kBtu/ft²)</th>                                        <td>   -0.7552</td> <td>    0.015</td> <td>  -51.008</td> <td> 0.000</td> <td>   -0.784</td> <td>   -0.726</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weather Normalized Site Electricity Intensity (kWh/ft²)</th>    <td>    1.2500</td> <td>    0.078</td> <td>   16.088</td> <td> 0.000</td> <td>    1.098</td> <td>    1.402</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Weather Normalized Site Natural Gas Intensity (therms/ft²)</th> <td>  -12.2602</td> <td>    1.320</td> <td>   -9.288</td> <td> 0.000</td> <td>  -14.848</td> <td>   -9.673</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Council District</th>                                           <td>   -0.0401</td> <td>    0.040</td> <td>   -0.991</td> <td> 0.322</td> <td>   -0.119</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>log_Direct GHG Emissions (Metric Tons CO2e)</th>                <td>   17.3042</td> <td>    0.227</td> <td>   76.197</td> <td> 0.000</td> <td>   16.859</td> <td>   17.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Borough_Queens</th>                                             <td>  -11.8745</td> <td>    1.368</td> <td>   -8.681</td> <td> 0.000</td> <td>  -14.556</td> <td>   -9.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Borough_Staten Island</th>                                      <td>  -17.9234</td> <td>    3.403</td> <td>   -5.266</td> <td> 0.000</td> <td>  -24.595</td> <td>  -11.251</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>123.809</td> <th>  Durbin-Watson:     </th> <td>   1.956</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 263.417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.019</td>  <th>  Prob(JB):          </th> <td>6.30e-58</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.002</td>  <th>  Cond. No.          </th> <td>6.98e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[3] The condition number is large, 6.98e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:                      y   R-squared (uncentered):                   0.845\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.845\n",
       "Method:                 Least Squares   F-statistic:                              4275.\n",
       "Date:                Thu, 27 Jul 2023   Prob (F-statistic):                        0.00\n",
       "Time:                        15:02:55   Log-Likelihood:                         -29532.\n",
       "No. Observations:                6285   AIC:                                  5.908e+04\n",
       "Df Residuals:                    6277   BIC:                                  5.913e+04\n",
       "Df Model:                           8                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "==============================================================================================================================\n",
       "                                                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------------------------\n",
       "index                                                          0.0036      0.000     17.676      0.000       0.003       0.004\n",
       "Site EUI (kBtu/ft²)                                           -0.7552      0.015    -51.008      0.000      -0.784      -0.726\n",
       "Weather Normalized Site Electricity Intensity (kWh/ft²)        1.2500      0.078     16.088      0.000       1.098       1.402\n",
       "Weather Normalized Site Natural Gas Intensity (therms/ft²)   -12.2602      1.320     -9.288      0.000     -14.848      -9.673\n",
       "Council District                                              -0.0401      0.040     -0.991      0.322      -0.119       0.039\n",
       "log_Direct GHG Emissions (Metric Tons CO2e)                   17.3042      0.227     76.197      0.000      16.859      17.749\n",
       "Borough_Queens                                               -11.8745      1.368     -8.681      0.000     -14.556      -9.193\n",
       "Borough_Staten Island                                        -17.9234      3.403     -5.266      0.000     -24.595     -11.251\n",
       "==============================================================================\n",
       "Omnibus:                      123.809   Durbin-Watson:                   1.956\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              263.417\n",
       "Skew:                           0.019   Prob(JB):                     6.30e-58\n",
       "Kurtosis:                       4.002   Cond. No.                     6.98e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[3] The condition number is large, 6.98e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and make the predictions by the model\n",
    "model_new2 = sm.OLS(list(y), train_df4).fit()\n",
    "model_new2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "__Observations__\n",
    "\n",
    "* All variables except Weather Normalized Site Electricity (kWh) have p-value < 0.05\n",
    "<br>\n",
    "\n",
    "* R squared value > 0.6\n",
    "<br>\n",
    "\n",
    "* Adj R squared close to R square\n",
    "<br>\n",
    "\n",
    "* Except Borough_Staten Island, all the other variables have +ve coefficient i.e. they have +ve relation with the dependent variable while Borough_Staten_Island have -ve relation which means increase in Borough_Staten_Island leads to low Scores and vice-versa\n",
    "<br>\n",
    "------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### <a id='toc1_2_1_7_'></a>[1.1.7 Regression Evaluation Metrics](#toc0_)\n",
    "\n",
    "Here are three common evaluation metrics for regression problems:\n",
    "\n",
    "* **Mean Absolute Error (MAE)** is the mean of the absolute value of the errors:  It is a measure of difference between two continuous variables. It is the simplest measure of forecast accuracy. MAE is simply, as the name suggests, is the mean of the absolute errors. The absolute error is the absolute value of the difference between the forecasted value and the actual value. MAE tells us how big of an error we can expect from the forecast on average.One problem with the MAE is that the relative size of the error is not always obvious. Sometimes it is hard to tell a big error from a small error. To deal with this problem, we can find the mean absolute error in percentage terms. Mean Absolute Percentage Error (MAPE) allows us to compare forecasts of different series in different scales. \n",
    "\n",
    "<br>\n",
    "\n",
    "* **Mean Squared Error (MSE)** is the mean of the squared errors: It measures the average of the squares of the errors—that is, the average squared difference between the estimated values and what is estimated. MSE) is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate.\n",
    "    The MSE is a measure of the quality of an estimator—it is always non-negative, and values closer to zero are better.\n",
    "    \n",
    "<br>\n",
    "\n",
    "* **Root Mean Squared Error (RMSE)** is the square root of the mean of the squared errors: The RMSE is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data–how close the observed data points are to the model’s predicted values. Whereas R-squared is a relative measure of fit, RMSE is an absolute measure of fit. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit. RMSE is a good measure of how accurately the model predicts the response, and it is the most important criterion for fit if the main purpose of the model is prediction. \n",
    "\n",
    "   <b> Quick Summary</b>\n",
    "    \n",
    "* MAE is the easiest to understand, because it's the average error.\n",
    "* MSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "* RMSE is even more popular than MSE, because RMSE is interpretable in the \"y\" units. All of these are loss functions, because we want to minimize them.\n",
    "\n",
    "    \n",
    "\\* More reference in the External Link Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 20.96115813035346\n",
      "MSE: 704.598947154503\n",
      "RMSE: 26.544282758336173\n"
     ]
    }
   ],
   "source": [
    "# Calculating Regression Evaulation metrics\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y, predictions))\n",
    "print('MSE:', metrics.mean_squared_error(y, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the model has high MAE & RMSE, and hence you can refine the model further by performing more iterations - i.e. including other variables and re-iterating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred):\n",
    "    return np.mean(abs(y_true - y_pred))\n",
    "\n",
    "def fit_and_evaluate(model):\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Make predictions and evalute\n",
    "    model_pred = model.predict(X_test)\n",
    "    model_mae = mae(y_test, model_pred)\n",
    "    \n",
    "    # Return the performance metric\n",
    "    return model_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[1.2 Model Validation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### <a id='toc1_2_2_1_'></a>[1.2.1 Variance vs Bias](#toc0_)\n",
    "\n",
    "The prediction error for any machine learning algorithm can be broken down into three parts:\n",
    "\n",
    "* __Bias Error:__ Bias are the simplifying assumptions made by a model to make the target function easier to learn.\n",
    "* __Variance Error:__ Variance is the amount that the estimate of the target function will change if different training data was used.\n",
    "* Irreducible Error\n",
    "\n",
    "The irreducible error cannot be reduced regardless of what algorithm is used. It is the error introduced from the chosen framing of the problem and may be caused by factors like unknown variables that influence the mapping of the input variables to the output variable.\n",
    "\n",
    "<img src='images/Variance_vs_bias.png' width=\"400\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "__In the above diagram, center of the target is a model that perfectly predicts correct values. As we move away from the bulls-eye our predictions become get worse and worse.__\n",
    "\n",
    "##### <a id='toc1_2_2_1_1_'></a>[1.2.1.1 Low Variance - Low Bias](#toc0_)\n",
    "\n",
    "<br> \n",
    "\n",
    "<img src='images/Desired.png' width=\"200\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "The model perfectly predicts correct values i.e. (y - predicted) has very low error.\n",
    "<br>\n",
    "\n",
    "##### <a id='toc1_2_2_1_2_'></a>[1.2.1.2 Low Variance - High Bias](#toc0_)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='images/Underfitting.png' width=\"200\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "Underfitting happens when a model unable to capture the underlying pattern of the data. It happens when we have very little amount of data to build an accurate model or when we try to build a linear model with a nonlinear data. __If our model is too simple and has very few parameters then it may have high bias and low variance__. It is usually the case in Linear Regression, Linear Discriminant Analysis, Logistic Regression etc\n",
    "<br>\n",
    "\n",
    "##### <a id='toc1_2_2_1_3_'></a>[1.2.1.3 High Variance - Low Bias](#toc0_)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='images/Overfitting.png' width=\"200\" height=\"200\" align=\"center\"/>\n",
    "\n",
    "Overfitting happens when our model captures the noise along with the underlying pattern in data. It happens when we train our model a lot over noisy dataset. __If our model has large number of parameters then it’s going to have high variance and low bias__. It is usually the case in Decision Trees, k-Nearest Neighbors, Support Vector Machines etc\n",
    "<br>\n",
    "\n",
    "##### <a id='toc1_2_2_1_4_'></a>[1.2.1.4 High Variance - High Bias](#toc0_)\n",
    "The model has not been able to learn anything from the data and hence throw random predicted values\n",
    "<br>\n",
    "\n",
    "\\* More reference in the External Link Library\n",
    "\n",
    "In reality since we can't have both low bias and low variance, so we want to aim for something in the middle\n",
    "\n",
    "<img src='images/biasvariance.png' width=\"500\" height=\"300\" align=\"center\"/>\n",
    "\n",
    "##### <a id='toc1_2_2_1_5_'></a>[1.2.1.5 Learning Curves](#toc0_)\n",
    "\n",
    "Let's say we have some data and split it into a training set and test set. We take one single instance (yes you read it correctly just 1 observation) from the training set and use it to estimate a model. Then we measure the model's error on the test set and on that single training instance. The error on the training instance will be 0, since it's quite easy to perfectly fit a single data point. The error on the test set, however, will be very large. That's because the model is built around a single instance, and it almost certainly won't be able to generalize accurately on data that hasn't seen before.\n",
    "\n",
    "Now let's say that instead of one training instance, we take ten and repeat the error measurements. Then we take fifty, one hundred, five hundred, until we use our entire training set. The error scores will vary more or less as we change the training set\n",
    "\n",
    "We thus have two error scores to monitor: one for the validation set, and one for the training sets. If we plot the evolution of the two error scores as training sets change, we end up with two curves. These are called _learning curves_. __In a nutshell, a learning curve shows how error changes as the training set size increases__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_3_'></a>[1.3 Polynomial Regression](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linearity is mathematically the nicest case that you can have. However, sometimes you may want to use higher order terms to see whether incorporating them might give you a better model for your phenomenon.\n",
    "\n",
    "In this lesson, we’ll take a look at polynomial regression.\n",
    "\n",
    "In polynomial regression with only one independent variable,we seek a regression model of the form\n",
    "f (x) = bₒ+ b₁x +b₂x²+ ⋯ + bₖxᵏ\n",
    "\n",
    "With multiple variables, for example,x(x₁,x₂) a quadratic model would take the form\n",
    "f (x₁,x₂) = bₒ + b₁x₁+ b₂x₂ + b₃x₁x₂ +b₄x₁x₂+b₅x₂²\n",
    "\n",
    "The term x₁,x₂ is called **a mixed term**\n",
    "\n",
    "Higher-order regression models with many variables can be written down similarly using multi-variate notation.\n",
    "\n",
    "In polynomial regression with only one independent variable, what we’re seeking is a regression model that contains not only the linear term, but also possibly a quadratic term, a cubic term, and then a term up to some higher order, say x to the power of k.\n",
    "\n",
    "One of the reasons why you may want to use a polynomial regression model is to take into account the fact that your data may not be linear. It may not be modeled well using a linear model, and so you want to take into account some possible non-linear effects using a polynomial regression model. Now, when you have multiple variables—for example, say that you have an input where you’ve got two components for the input—then a quadratic model would take this form.\n",
    "\n",
    "In the next section why it would be wise to use Polynomial Regression over Linear Regression in some cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is applied for a data set when the values are linear. For example:\n",
    "\n",
    "<br>\n",
    "<img src='images/LR1.jpg' width=\"600\" height=\"400\" align=\"center\"/>\n",
    "<br>\n",
    "\n",
    "And real life is not that simple, especially when you observe from many different companies in different industries. Salary of 1 YE business analyst is different from 1 YE engineer; even 1 YE civil engineer is different from computer engineer; and if you compare 2 civil engineers from 2 different companies, their salary mostly different as well. So how can we predict the salary of a candidate?\n",
    "\n",
    "We are going to use the below dataset to represent a polynomial shape\n",
    "\n",
    "<br>\n",
    "<img src='images/Salary.jpg' width=\"500\" height=\"400\" align=\"center\"/>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an overview of the increment of salary, let’s visualize the data set into a chart:\n",
    "\n",
    "<br>\n",
    "<img src='images/salary_YE.png' width=\"600\" height=\"400\" align=\"center\"/>\n",
    "<br>\n",
    "\n",
    "According to the picture above, the salary range of a candidate with 5.5. YE could be approximately from 0 to $200,000. Why? Look, the salary observations in this scenarios are not linear. They are in a curved shape! That’s why applying Linear Regression in this scenario is not giving you the right value. It’s time for Polynomial Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply Polynomial Regression on the avove set to see how the result differs from Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv(r'Data/position_salaries.csv')\n",
    "X = dataset.iloc[:, 1:2].values\n",
    "y = dataset.iloc[:, 2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_3_1_'></a>[1.3.1 Code explanation:](#toc0_)\n",
    "\n",
    "**Dataset**: the table contains all values in our csv file\n",
    "X: the 2nd column which contains Years Experience array\n",
    "y: the last column which contains Salary array\n",
    "Let’s split our dataset to get training set and testing set (both X and y values per each set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**test_size=0.2**: we will split our dataset (10 observations) into 2 parts (training set, test set) and the ratio of test set compare to dataset is 0.2 (2 observations will be put into the test set. You can put it 1/5 to get 20% or 0.2, they are the same. We should not let the test set too big; if it’s too big, we will lack of data to train. Normally, we should pick around 5% to 30%.\n",
    "\n",
    "**train_size**: if we use the test_size already, the rest of data will automatically be assigned to train_size.\n",
    "random_state: this is the seed for the random number generator. We can put an instance of the RandomState class as well. If we leave it blank or 0, the RandomState instance used by np.random will be used instead.\n",
    "\n",
    "We already have the train set and test set, now we have to build the Regression Model. Firstly, we will build a Linear Regression model and visualize it (it’s no need to include this step in your practice, we just do this for comparison between Linear and Polynomial only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Linear Regression to the dataset\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the Linear Regression results\n",
    "def viz_linear():\n",
    "    plt.scatter(X, y, color='red')\n",
    "    plt.plot(X, lin_reg.predict(X), color='blue')\n",
    "    plt.title('Truth or Bluff (Linear Regression)')\n",
    "    plt.xlabel('Position level')\n",
    "    plt.ylabel('Salary')\n",
    "    plt.show()\n",
    "    return\n",
    "viz_linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In another hand, we will build the Polynomial Regression model and visualize it to see the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Polynomial Regression to the dataset\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_reg = PolynomialFeatures(degree=4)\n",
    "X_poly = poly_reg.fit_transform(X)\n",
    "pol_reg = LinearRegression()\n",
    "pol_reg.fit(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the Polymonial Regression results\n",
    "def viz_polymonial():\n",
    "    plt.scatter(X, y, color='red')\n",
    "    plt.plot(X, pol_reg.predict(poly_reg.fit_transform(X)), color='blue')\n",
    "    plt.title('Truth or Bluff (Linear Regression)')\n",
    "    plt.xlabel('Position level')\n",
    "    plt.ylabel('Salary')\n",
    "    plt.show()\n",
    "    return\n",
    "viz_polymonial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([439200.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting a new result with Linear Regression\n",
    "lin_reg.predict([[5.5]])\n",
    "#output should be 439200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([164562.49999996])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting a new result with Polymonial Regression\n",
    "pol_reg.predict(poly_reg.fit_transform([[5.5]]))\n",
    "#output should be 164562.49999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, the predicted values using Linear Regression model and Polynomial Regression model are totally different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the  data set, the salary of the candidate should be:\n",
    "\n",
    "$0 < salary < $250,000\n",
    "\n",
    "But the predicted salary using Linear Regression lin_reg is `$439,200`.\n",
    "It is unacceptable!\n",
    "What is about using Polynomial Regression? Our pol_reg value is `$164,562.99` which is very close to our Mean value of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_4_'></a>[1.4 Summary](#toc0_)\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size = 3>\n",
    "    \n",
    "__What we have learned today__\n",
    "\n",
    "* If your value of R-squared  is large, you have a better chance of your regression model fitting the observations. \n",
    "\n",
    "* A p-value less than 0.05 is typically considered to be statistically significant, in which case the null hypothesis should be rejected. \n",
    "\n",
    "* There should be no multicollinearity or heteroscedasticity \n",
    "\n",
    "* Bias and Variance is always a balance i.e., higher bias leads to lower variance. The goal is to have low for both, but in real world, it is a balance\n",
    "  \n",
    "* There could be situations when Polynomial Regression would be a better choice over Linear Regression based on the dataset\n",
    "\n",
    "Finally, remember to K.I.S.S. (keep it simple...) If two models are generally similar in terms of their error statistics and other diagnostics, you should prefer the one that is simpler and/or easier to understand. The simpler model is likely to be closer to the truth, and it will usually be more easily accepted by others.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                **************************> This is the end of our session ******************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2023 by Boston Consulting Group. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
